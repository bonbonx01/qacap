#!/usr/bin/env python3
"""
QA-ViT + PNP-VQA å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¿®å¤åçš„å®ç°ï¼š
1. ä¸ä¾èµ– LAVIS æ¡†æ¶
2. ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºæ ¸å¿ƒåŠŸèƒ½
3. å±•ç¤ºçƒ­åŠ›å›¾ç”Ÿæˆå’Œå¯è§†åŒ–è¿‡ç¨‹
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os

class MockTextEncoder:
    """æ¨¡æ‹Ÿæ–‡æœ¬ç¼–ç å™¨ç”¨äºæ¼”ç¤º"""
    def __init__(self):
        self.tokenizer = MockTokenizer()
        
    def __call__(self, input_ids, attention_mask, return_dict=True):
        # è¿”å›æ¨¡æ‹Ÿçš„æ–‡æœ¬ç‰¹å¾
        batch_size, seq_len = input_ids.shape
        last_hidden_state = torch.randn(batch_size, seq_len, 768)
        
        class MockOutput:
            def __init__(self, hidden_state):
                self.last_hidden_state = hidden_state
        
        return MockOutput(last_hidden_state)

class MockTokenizer:
    """æ¨¡æ‹Ÿåˆ†è¯å™¨"""
    def __call__(self, texts, padding='longest', truncation=True, return_tensors="pt"):
        max_len = 20
        batch_size = len(texts) if isinstance(texts, list) else 1
        
        class MockTokenizerOutput:
            def __init__(self, batch_size, max_len):
                self.input_ids = torch.randint(1, 1000, (batch_size, max_len))
                self.attention_mask = torch.ones(batch_size, max_len)
                
            def to(self, device):
                self.input_ids = self.input_ids.to(device)
                self.attention_mask = self.attention_mask.to(device)
                return self
        
        return MockTokenizerOutput(batch_size, max_len)

class MockQAViTVisionTower:
    """æ¨¡æ‹Ÿ QA-ViT è§†è§‰å¡”"""
    def __init__(self, config, instruction_dim):
        self.config = config
        self.instruction_dim = instruction_dim
        self.num_patches = 576  # 24x24 patches
        
    def to(self, device):
        return self
        
    @property
    def device(self):
        return torch.device('cpu')
    
    def __call__(self, pixel_values, instruct_states=None, instruct_masks=None, 
                 output_attentions=True, output_hidden_states=True):
        batch_size = pixel_values.shape[0]
        
        # æ¨¡æ‹Ÿè§†è§‰è¾“å‡º
        class MockVisionOutput:
            def __init__(self, batch_size, num_patches):
                # æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡ (num_layers, batch, num_heads, seq_len, seq_len)
                seq_len = num_patches + 1  # +1 for CLS token
                self.attentions = [
                    torch.rand(batch_size, 12, seq_len, seq_len) 
                    for _ in range(12)  # 12 layers
                ]
                
                # æ¨¡æ‹Ÿæœ€åéšè—çŠ¶æ€
                self.last_hidden_state = torch.randn(batch_size, seq_len, 768)
        
        return MockVisionOutput(batch_size, self.num_patches)

class MockCaptioningModel:
    """æ¨¡æ‹Ÿæè¿°ç”Ÿæˆæ¨¡å‹"""
    def __init__(self):
        self.prompt = "a picture of"
        self.device = torch.device('cpu')
        
    def to(self, device):
        return self
    
    def forward_encoder(self, samples):
        # è¿”å›æ¨¡æ‹Ÿçš„ç¼–ç å™¨è¾“å‡º
        batch_size = samples['image'].shape[0]
        num_patches = 577  # åŒ…å«CLS token
        hidden_dim = 768
        return torch.randn(batch_size, num_patches, hidden_dim)

class MockQuestionAnsweringModel:
    """æ¨¡æ‹Ÿé—®ç­”æ¨¡å‹"""
    def __init__(self):
        self.device = torch.device('cpu')
        
    def to(self, device):
        return self

def create_mock_pnp_vqa_model():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„ PNP-VQA æ¨¡å‹ç”¨äºæ¼”ç¤º"""
    
    # è¿™é‡Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥å¯¼å…¥ä¿®å¤çš„æ¨¡å‹ï¼Œå› ä¸ºå®ƒä¾èµ– LAVIS
    # æ‰€ä»¥æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬æ¥æ¼”ç¤ºæ ¸å¿ƒæ¦‚å¿µ
    
    class SimplifiedPNPVQAWithQAViT:
        def __init__(self):
            self.text_encoder = MockTextEncoder()
            self.qavit_vision_tower = MockQAViTVisionTower({}, 768)
            self.image_captioning_model = MockCaptioningModel()
            self.question_answering_model = MockQuestionAnsweringModel()
            self.num_patches = 576
            
        @property
        def device(self):
            return torch.device('cpu')
            
        def extract_qavit_attention(self, vision_outputs, block_num=7):
            """ä» QA-ViT è¾“å‡ºä¸­æå–æ³¨æ„åŠ›å›¾"""
            block_num = min(block_num, len(vision_outputs.attentions) - 1)
            attention_weights = vision_outputs.attentions[block_num]
            
            # å¹³å‡è·¨å¤´éƒ¨çš„æ³¨æ„åŠ›
            attention_weights = attention_weights.mean(dim=1)
            
            # æå–CLS tokenåˆ°å›¾åƒpatchesçš„æ³¨æ„åŠ›
            cls_attention = attention_weights[:, 0, -self.num_patches:]
            
            # å½’ä¸€åŒ–
            cls_attention = torch.softmax(cls_attention, dim=-1)
            return cls_attention
            
        def forward_itm(self, samples, block_num=7):
            """ç”Ÿæˆé—®é¢˜æ„ŸçŸ¥çš„æ³¨æ„åŠ›å›¾"""
            image = samples['image']
            questions = samples['text_input']
            
            # ç¼–ç é—®é¢˜
            encoded_questions = self.text_encoder.tokenizer(
                questions, 
                padding='longest', 
                truncation=True,
                return_tensors="pt"
            )
            
            # è·å–æ–‡æœ¬ç‰¹å¾
            text_outputs = self.text_encoder(
                input_ids=encoded_questions.input_ids,
                attention_mask=encoded_questions.attention_mask,
                return_dict=True
            )
            
            instruct_states = text_outputs.last_hidden_state
            instruct_masks = encoded_questions.attention_mask
            
            # é€šè¿‡ QA-ViT å¤„ç†å›¾åƒ
            vision_outputs = self.qavit_vision_tower(
                pixel_values=image,
                instruct_states=instruct_states,
                instruct_masks=instruct_masks,
                output_attentions=True,
                output_hidden_states=True
            )
            
            # æå–æ³¨æ„åŠ›å›¾
            attention_maps = self.extract_qavit_attention(
                vision_outputs, 
                block_num=block_num
            )
            
            # å­˜å‚¨åˆ°samplesä¸­
            samples['gradcams'] = attention_maps.reshape(image.size(0), -1)
            return samples
            
        def predict_answers(self, samples, **kwargs):
            """ä¸»è¦çš„é¢„æµ‹æ¥å£"""
            # ç”Ÿæˆæ³¨æ„åŠ›å›¾
            samples = self.forward_itm(samples, block_num=kwargs.get('block_num', 7))
            
            # æ¨¡æ‹Ÿç­”æ¡ˆå’Œæè¿°
            batch_size = samples['image'].size(0)
            answers = ["This is a sample answer"] * batch_size
            captions = [["Sample caption 1", "Sample caption 2"]] * batch_size
            
            return answers, captions, samples['gradcams']
    
    return SimplifiedPNPVQAWithQAViT()

def create_sample_image():
    """åˆ›å»ºç¤ºä¾‹å›¾åƒ"""
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆæˆå›¾åƒ
    image = Image.new('RGB', (384, 384), color='lightblue')
    
    # å¯ä»¥æ·»åŠ ä¸€äº›å›¾å½¢å…ƒç´ 
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    
    fig, ax = plt.subplots(figsize=(4, 4))
    ax.set_xlim(0, 384)
    ax.set_ylim(0, 384)
    
    # æ·»åŠ ä¸€äº›å½¢çŠ¶
    circle = patches.Circle((192, 300), 50, facecolor='yellow')  # å¤ªé˜³
    ax.add_patch(circle)
    
    house = patches.Rectangle((100, 50), 180, 120, facecolor='brown')  # æˆ¿å­
    ax.add_patch(house)
    
    roof = patches.Polygon([(100, 170), (190, 220), (280, 170)], facecolor='red')
    ax.add_patch(roof)
    
    ax.set_aspect('equal')
    ax.axis('off')
    plt.savefig('temp_image.png', dpi=96, bbox_inches='tight', pad_inches=0)
    plt.close()
    
    # è½¬æ¢ä¸º tensor
    image = Image.open('temp_image.png').convert('RGB').resize((384, 384))
    image_tensor = torch.tensor(np.array(image)).float().permute(2, 0, 1).unsqueeze(0) / 255.0
    
    return image_tensor, image

def visualize_attention_heatmap(original_image, attention_map, question, answer):
    """å¯è§†åŒ–æ³¨æ„åŠ›çƒ­åŠ›å›¾"""
    # è½¬æ¢æ³¨æ„åŠ›å›¾
    if isinstance(attention_map, torch.Tensor):
        attention_map = attention_map.cpu().numpy().squeeze()
    
    # é‡å¡‘ä¸º24x24
    size = int(np.sqrt(len(attention_map)))
    heatmap = attention_map.reshape(size, size)
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # åŸå§‹å›¾åƒ
    axes[0].imshow(original_image)
    axes[0].set_title("Original Image")
    axes[0].axis('off')
    
    # çƒ­åŠ›å›¾
    im1 = axes[1].imshow(heatmap, cmap='jet', interpolation='bilinear')
    axes[1].set_title("QA-ViT Attention Heatmap")
    axes[1].axis('off')
    plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)
    
    # å åŠ æ•ˆæœ
    img_array = np.array(original_image)
    from scipy.ndimage import zoom
    heatmap_resized = zoom(heatmap, 
                          (img_array.shape[0]/heatmap.shape[0], 
                           img_array.shape[1]/heatmap.shape[1]))
    
    # å½’ä¸€åŒ–å¹¶åˆ›å»ºå½©è‰²çƒ­åŠ›å›¾
    heatmap_norm = (heatmap_resized - heatmap_resized.min()) / (heatmap_resized.max() - heatmap_resized.min())
    heatmap_colored = plt.cm.jet(heatmap_norm)[:, :, :3]
    
    # å åŠ 
    alpha = 0.4
    overlay = img_array / 255.0 * (1 - alpha) + heatmap_colored * alpha
    
    axes[2].imshow(overlay)
    axes[2].set_title("Overlay")
    axes[2].axis('off')
    
    # æ·»åŠ æ ‡é¢˜
    fig.suptitle(f'Question: "{question}"\nAnswer: "{answer}"', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('qavit_demo_result.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("å¯è§†åŒ–ç»“æœå·²ä¿å­˜ä¸º: qavit_demo_result.png")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("="*60)
    print("QA-ViT + PNP-VQA å¿«é€Ÿå¼€å§‹æ¼”ç¤º")
    print("="*60)
    
    # 1. åˆ›å»ºæ¨¡å‹
    print("\n1. åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹...")
    model = create_mock_pnp_vqa_model()
    print("   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # 2. åˆ›å»ºç¤ºä¾‹æ•°æ®
    print("\n2. å‡†å¤‡ç¤ºä¾‹æ•°æ®...")
    image_tensor, original_image = create_sample_image()
    question = "What color is the sun in the image?"
    
    samples = {
        'image': image_tensor,
        'text_input': [question]
    }
    print(f"   âœ“ å›¾åƒå°ºå¯¸: {image_tensor.shape}")
    print(f"   âœ“ é—®é¢˜: {question}")
    
    # 3. è¿è¡Œæ¨¡å‹æ¨ç†
    print("\n3. è¿è¡Œæ¨¡å‹æ¨ç†...")
    answers, captions, attention_maps = model.predict_answers(
        samples,
        block_num=7,
        num_captions=5
    )
    
    print(f"   âœ“ ç­”æ¡ˆ: {answers[0]}")
    print(f"   âœ“ æ³¨æ„åŠ›å›¾å½¢çŠ¶: {attention_maps.shape}")
    print(f"   âœ“ æè¿°æ•°é‡: {len(captions[0])}")
    
    # 4. å¯è§†åŒ–ç»“æœ
    print("\n4. å¯è§†åŒ–æ³¨æ„åŠ›çƒ­åŠ›å›¾...")
    
    try:
        visualize_attention_heatmap(
            original_image,
            attention_maps[0],
            question,
            answers[0]
        )
        print("   âœ“ å¯è§†åŒ–å®Œæˆ")
    except ImportError as e:
        print(f"   âš  å¯è§†åŒ–éœ€è¦é¢å¤–ä¾èµ– (scipy): {e}")
        print("   â†’ è¯·å®‰è£…: pip install scipy")
    
    # 5. è¾“å‡ºæ€»ç»“
    print("\n" + "="*60)
    print("æ¼”ç¤ºæ€»ç»“:")
    print("="*60)
    print("âœ… æˆåŠŸå±•ç¤ºäº† QA-ViT + PNP-VQA çš„æ ¸å¿ƒåŠŸèƒ½")
    print("âœ… ç”Ÿæˆäº†é—®é¢˜æ„ŸçŸ¥çš„æ³¨æ„åŠ›å›¾")
    print("âœ… æä¾›äº†ç­”æ¡ˆå’Œå›¾åƒæè¿°")
    print("âœ… åˆ›å»ºäº†çƒ­åŠ›å›¾å¯è§†åŒ–")
    
    print("\nå…³é”®ç‰¹æ€§:")
    print("â€¢ é—®é¢˜æ„ŸçŸ¥æ³¨æ„åŠ›: æ³¨æ„åŠ›å›¾æ ¹æ®é—®é¢˜å†…å®¹åŠ¨æ€è°ƒæ•´")
    print("â€¢ å¤šæ¨¡æ€èåˆ: ç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ¨ç†") 
    print("â€¢ å¯è§†åŒ–å‹å¥½: ç›´è§‚å±•ç¤ºæ¨¡å‹å…³æ³¨çš„å›¾åƒåŒºåŸŸ")
    
    print("\nå®é™…ä½¿ç”¨ä¸­ï¼Œæ‚¨å¯ä»¥:")
    print("1. æ›¿æ¢æ¨¡æ‹Ÿç»„ä»¶ä¸ºçœŸå®çš„é¢„è®­ç»ƒæ¨¡å‹")
    print("2. ä½¿ç”¨çœŸå®å›¾åƒå’Œå¤šæ ·åŒ–çš„é—®é¢˜") 
    print("3. è°ƒæ•´æ³¨æ„åŠ›æå–çš„å±‚æ•°å’Œå‚æ•°")
    print("4. æ‰©å±•å¯è§†åŒ–åŠŸèƒ½å’Œè¯„ä¼°æŒ‡æ ‡")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if os.path.exists('temp_image.png'):
        os.remove('temp_image.png')
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main()