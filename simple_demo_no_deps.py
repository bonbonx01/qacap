#!/usr/bin/env python3
"""
QA-ViT + PNP-VQA æ¦‚å¿µæ¼”ç¤º (æ— å¤–éƒ¨ä¾èµ–)

è¿™ä¸ªæ¼”ç¤ºè„šæœ¬ä¸ä¾èµ–ä»»ä½•å¤–éƒ¨åº“ï¼Œçº¯ç²¹ç”¨äºå±•ç¤ºï¼š
1. QA-ViT + PNP-VQA çš„æ ¸å¿ƒæ¦‚å¿µ
2. æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†
3. æ¨¡å‹æ¶æ„å’Œæ•°æ®æµç¨‹
"""

import random
import math

def simulate_attention_map(question, image_regions):
    """
    æ¨¡æ‹Ÿ QA-ViT æ ¹æ®é—®é¢˜ç”Ÿæˆçš„æ³¨æ„åŠ›å›¾
    
    Args:
        question: é—®é¢˜æ–‡æœ¬
        image_regions: å›¾åƒåŒºåŸŸæè¿°åˆ—è¡¨
        
    Returns:
        attention_weights: å„åŒºåŸŸçš„æ³¨æ„åŠ›æƒé‡
    """
    print(f"ğŸ§  QA-ViT æ­£åœ¨åˆ†æé—®é¢˜: '{question}'")
    
    # ç®€å•çš„å…³é”®è¯åŒ¹é…æ¥æ¨¡æ‹Ÿé—®é¢˜æ„ŸçŸ¥
    attention_weights = []
    question_lower = question.lower()
    
    for region in image_regions:
        region_lower = region.lower()
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        relevance = 0.1  # åŸºç¡€æ³¨æ„åŠ›
        
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        question_words = question_lower.split()
        region_words = region_lower.split()
        
        for q_word in question_words:
            for r_word in region_words:
                if q_word == r_word or q_word in r_word or r_word in q_word:
                    relevance += 0.3
        
        # æ·»åŠ ä¸€äº›éšæœºæ€§æ¥æ¨¡æ‹Ÿå¤æ‚çš„è¯­ä¹‰ç†è§£
        relevance += random.uniform(0, 0.2)
        attention_weights.append(relevance)
    
    # å½’ä¸€åŒ–æ³¨æ„åŠ›æƒé‡ï¼ˆsoftmaxï¼‰
    max_weight = max(attention_weights)
    exp_weights = [math.exp(w - max_weight) for w in attention_weights]
    sum_exp = sum(exp_weights)
    normalized_weights = [w / sum_exp for w in exp_weights]
    
    return normalized_weights

def generate_answer_and_captions(question, image_regions, attention_weights):
    """
    åŸºäºæ³¨æ„åŠ›æƒé‡ç”Ÿæˆç­”æ¡ˆå’Œæè¿°
    
    Args:
        question: é—®é¢˜æ–‡æœ¬
        image_regions: å›¾åƒåŒºåŸŸæè¿°
        attention_weights: æ³¨æ„åŠ›æƒé‡
        
    Returns:
        answer: é¢„æµ‹ç­”æ¡ˆ
        captions: ç”Ÿæˆçš„æè¿°åˆ—è¡¨
    """
    print("ğŸ’¬ PNP-VQA æ­£åœ¨ç”Ÿæˆç­”æ¡ˆå’Œæè¿°...")
    
    # æ‰¾åˆ°æœ€å—å…³æ³¨çš„åŒºåŸŸ
    max_attention_idx = attention_weights.index(max(attention_weights))
    focused_region = image_regions[max_attention_idx]
    
    # åŸºäºé—®é¢˜ç±»å‹å’Œç„¦ç‚¹åŒºåŸŸç”Ÿæˆç­”æ¡ˆ
    question_lower = question.lower()
    
    if "what color" in question_lower:
        if "sun" in focused_region.lower():
            answer = "yellow"
        elif "sky" in focused_region.lower():
            answer = "blue"
        elif "grass" in focused_region.lower():
            answer = "green"
        else:
            answer = "multiple colors"
    elif "where" in question_lower:
        answer = f"in the {focused_region.lower()}"
    elif "how many" in question_lower:
        answer = "one" if "sun" in question_lower else "several"
    else:
        answer = f"related to {focused_region.lower()}"
    
    # ç”ŸæˆåŸºäºæ³¨æ„åŠ›çš„æè¿°
    captions = []
    for i, (region, weight) in enumerate(zip(image_regions, attention_weights)):
        if weight > 0.15:  # åªä¸ºé«˜æ³¨æ„åŠ›åŒºåŸŸç”Ÿæˆæè¿°
            captions.append(f"A {region.lower()} with attention weight {weight:.2f}")
    
    return answer, captions

def visualize_attention_text(image_regions, attention_weights):
    """
    ä»¥æ–‡æœ¬å½¢å¼å¯è§†åŒ–æ³¨æ„åŠ›åˆ†å¸ƒ
    
    Args:
        image_regions: å›¾åƒåŒºåŸŸæè¿°
        attention_weights: æ³¨æ„åŠ›æƒé‡
    """
    print("\nğŸ“Š æ³¨æ„åŠ›åˆ†å¸ƒå¯è§†åŒ–:")
    print("-" * 50)
    
    max_width = 30
    for region, weight in zip(image_regions, attention_weights):
        bar_length = int(weight * max_width)
        bar = "â–ˆ" * bar_length + "â–‘" * (max_width - bar_length)
        print(f"{region:15} â”‚{bar}â”‚ {weight:.3f}")
    
    print("-" * 50)

def demonstrate_qavit_pnp():
    """ä¸»è¦çš„æ¼”ç¤ºå‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ QA-ViT + PNP-VQA æ¦‚å¿µæ¼”ç¤º")
    print("=" * 60)
    
    # å®šä¹‰ä¸€ä¸ªç¤ºä¾‹åœºæ™¯
    print("\nğŸ–¼ï¸  æ¨¡æ‹Ÿå›¾åƒåœºæ™¯:")
    image_regions = [
        "bright yellow sun",
        "blue sky with clouds",
        "green grass field", 
        "brown wooden house",
        "red triangular roof",
        "tall green tree",
        "white fluffy clouds",
        "stone pathway"
    ]
    
    for i, region in enumerate(image_regions):
        print(f"   åŒºåŸŸ {i+1}: {region}")
    
    # å®šä¹‰ç¤ºä¾‹é—®é¢˜
    questions = [
        "What color is the sun?",
        "Where is the house located?", 
        "How many trees are there?",
        "What is in the sky?"
    ]
    
    print(f"\nğŸ” å‡†å¤‡å›ç­” {len(questions)} ä¸ªé—®é¢˜...")
    
    # å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œæ¼”ç¤º
    for i, question in enumerate(questions):
        print(f"\n{'=' * 20} é—®é¢˜ {i+1} {'=' * 20}")
        print(f"â“ é—®é¢˜: {question}")
        
        # 1. ç”Ÿæˆæ³¨æ„åŠ›å›¾
        attention_weights = simulate_attention_map(question, image_regions)
        
        # 2. å¯è§†åŒ–æ³¨æ„åŠ›
        visualize_attention_text(image_regions, attention_weights)
        
        # 3. ç”Ÿæˆç­”æ¡ˆå’Œæè¿°  
        answer, captions = generate_answer_and_captions(
            question, image_regions, attention_weights
        )
        
        print(f"\nâœ… é¢„æµ‹ç­”æ¡ˆ: {answer}")
        print(f"ğŸ“ ç”Ÿæˆçš„æè¿°:")
        for j, caption in enumerate(captions):
            print(f"   {j+1}. {caption}")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        max_attention = max(attention_weights)
        focused_region_idx = attention_weights.index(max_attention)
        
        print(f"\nğŸ“ˆ å…³é”®ç»Ÿè®¡:")
        print(f"   æœ€å…³æ³¨åŒºåŸŸ: {image_regions[focused_region_idx]}")
        print(f"   æœ€å¤§æ³¨æ„åŠ›å€¼: {max_attention:.3f}")
        print(f"   å¹³å‡æ³¨æ„åŠ›å€¼: {sum(attention_weights)/len(attention_weights):.3f}")
        
        if i < len(questions) - 1:
            print(f"\n{'â³ å‡†å¤‡ä¸‹ä¸€ä¸ªé—®é¢˜...'}")
    
    # æ€»ç»“æ¼”ç¤º
    print(f"\n{'=' * 60}")
    print("ğŸ¯ æ¼”ç¤ºæ€»ç»“")
    print("=" * 60)
    print("âœ¨ æ ¸å¿ƒç‰¹æ€§å±•ç¤º:")
    print("  â€¢ é—®é¢˜æ„ŸçŸ¥æ³¨æ„åŠ›: ä¸åŒé—®é¢˜äº§ç”Ÿä¸åŒçš„æ³¨æ„åŠ›åˆ†å¸ƒ")
    print("  â€¢ å¤šæ¨¡æ€èåˆ: ç»“åˆè§†è§‰åŒºåŸŸä¿¡æ¯å’Œæ–‡æœ¬é—®é¢˜")
    print("  â€¢ åŠ¨æ€ç„¦ç‚¹: æ ¹æ®é—®é¢˜å†…å®¹è‡ªåŠ¨å…³æ³¨ç›¸å…³åŒºåŸŸ")
    print("  â€¢ æè¿°ç”Ÿæˆ: åŸºäºæ³¨æ„åŠ›æƒé‡ç”Ÿæˆç›¸å…³æè¿°")
    
    print(f"\nğŸ”§ æŠ€æœ¯å®ç°è¦ç‚¹:")
    print("  â€¢ QA-ViT æ›¿æ¢ä¼ ç»Ÿ GradCAM æä¾›æ›´ç²¾å‡†çš„æ³¨æ„åŠ›")
    print("  â€¢ æ³¨æ„åŠ›æƒé‡æŒ‡å¯¼åç»­çš„æè¿°ç”Ÿæˆå’Œé—®ç­”")
    print("  â€¢ æ”¯æŒå¤šç§é—®é¢˜ç±»å‹å’Œå¤æ‚åœºæ™¯ç†è§£")
    print("  â€¢ æä¾›å¯è§†åŒ–å’Œå¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹")
    
    print(f"\nğŸ“ ç›¸å…³æ–‡ä»¶:")
    print("  â€¢ qacap_vqa_fixed.py - ä¿®å¤åçš„å®Œæ•´å®ç°")
    print("  â€¢ qavit_pnp_demo.py - å®Œæ•´åŠŸèƒ½æ¼”ç¤ºè„šæœ¬") 
    print("  â€¢ demo_config.yaml - é…ç½®æ–‡ä»¶")
    print("  â€¢ README_QAViT_PNP.md - è¯¦ç»†è¯´æ˜æ–‡æ¡£")
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›æ¨¡æ‹Ÿçš„ç»„ä»¶ä¼šè¢«çœŸå®çš„ç¥ç»ç½‘ç»œæ¨¡å‹æ›¿æ¢ã€‚")

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥è·å¾—å¯é‡ç°çš„ç»“æœ
    random.seed(42)
    demonstrate_qavit_pnp()